Index: mpeg2dec-0.5.1/configure.ac
===================================================================
--- mpeg2dec-0.5.1.orig/configure.ac
+++ mpeg2dec-0.5.1/configure.ac
@@ -105,7 +105,7 @@ elif test x"$GCC" = x"yes"; then
 	AC_LANG(C)
 	AC_COMPILE_IFELSE(
 		[AC_LANG_SOURCE([[
-			void foo(void) { __asm__ volatile("pld [r1]"); }]])],
+			void foo(void) { __asm__ volatile("@pld [r1]"); }]])],
 		build_arm_opt=true; AC_DEFINE([ARCH_ARM],,[ARM architecture]),
 		build_arm_opt=false);;
     esac
Index: mpeg2dec-0.5.1/libmpeg2/motion_comp_arm_s.S
===================================================================
--- mpeg2dec-0.5.1.orig/libmpeg2/motion_comp_arm_s.S
+++ mpeg2dec-0.5.1/libmpeg2/motion_comp_arm_s.S
@@ -27,7 +27,7 @@
 	.internal MC_put_o_16_arm
 MC_put_o_16_arm:
 	@@ void func(uint8_t * dest, const uint8_t * ref, int stride, int height)
-	pld [r1]
+	@pld [r1]
         stmfd sp!, {r4-r11, lr} @ R14 is also called LR
 	and r4, r1, #3
 	ldrb r4, [pc, r4]
@@ -41,7 +41,7 @@ MC_put_o_16_arm:
 MC_put_o_16_arm_align0:
 	ldmia r1, {r4-r7}
 	add r1, r1, r2
-	pld [r1]
+	@pld [r1]
 	stmia r0, {r4-r7}
 	subs r3, r3, #1
 	add r0, r0, r2
@@ -52,7 +52,7 @@ MC_put_o_16_arm_align0:
 	ldmia r1, {r4-r8}
 	add r1, r1, r2
 	mov r9, r4, lsr #(\shift)
-	pld [r1]
+	@pld [r1]
 	mov r10, r5, lsr #(\shift)
 	orr r9, r9, r5, lsl #(32-\shift)
 	mov r11, r6, lsr #(\shift)
@@ -87,7 +87,7 @@ MC_put_o_16_arm_align3:
 	.internal MC_put_o_8_arm
 MC_put_o_8_arm:
 	@@ void func(uint8_t * dest, const uint8_t * ref, int stride, int height)
-	pld [r1]
+	@pld [r1]
         stmfd sp!, {r4-r10, lr} @ R14 is also called LR
 	and r4, r1, #3
 	ldrb r4, [pc, r4]
@@ -101,7 +101,7 @@ MC_put_o_8_arm:
 MC_put_o_8_arm_align0:
 	ldmia r1, {r4-r5}
 	add r1, r1, r2
-	pld [r1]
+	@pld [r1]
 	stmia r0, {r4-r5}
 	add r0, r0, r2
 	subs r3, r3, #1
@@ -112,7 +112,7 @@ MC_put_o_8_arm_align0:
 	ldmia r1, {r4-r6}
 	add r1, r1, r2
 	mov r9, r4, lsr #(\shift)
-	pld [r1]
+	@pld [r1]
 	mov r10, r5, lsr #(\shift)
 	orr r9, r9, r5, lsl #(32-\shift)
 	orr r10, r10, r6, lsl #(32-\shift)
@@ -156,7 +156,7 @@ MC_put_o_8_arm_align3:
 	.internal MC_put_x_16_arm
 MC_put_x_16_arm:
 	@@ void func(uint8_t * dest, const uint8_t * ref, int stride, int height)
-	pld [r1]
+	@pld [r1]
         stmfd sp!, {r4-r11,lr} @ R14 is also called LR
 	ldr r11, 0f
 	and r4, r1, #3
@@ -186,7 +186,7 @@ MC_put_x_16_arm:
 MC_put_x_16_arm_align0:
 	ldmia r1, {r4-r8}
 	add r1, r1, r2
-	pld [r1]
+	@pld [r1]
 	AVG_PW r7, r8
 	AVG_PW r6, r7
 	AVG_PW r5, r6
@@ -200,7 +200,7 @@ MC_put_x_16_arm_align1:
 	and r1, r1, #0xFFFFFFFC
 1:	ldmia r1, {r4-r8}
 	add r1, r1, r2
-	pld [r1]
+	@pld [r1]
 	ADJ_ALIGN_QW 8, r4, r5, r6, r7, r8
 	AVG_PW r7, r8
 	AVG_PW r6, r7
@@ -215,7 +215,7 @@ MC_put_x_16_arm_align2:
 	and r1, r1, #0xFFFFFFFC
 1:	ldmia r1, {r4-r8}
 	add r1, r1, r2
-	pld [r1]
+	@pld [r1]
 	ADJ_ALIGN_QW 16, r4, r5, r6, r7, r8
 	AVG_PW r7, r8
 	AVG_PW r6, r7
@@ -230,7 +230,7 @@ MC_put_x_16_arm_align3:
 	and r1, r1, #0xFFFFFFFC
 1:	ldmia r1, {r4-r8}
 	add r1, r1, r2
-	pld [r1]
+	@pld [r1]
 	ADJ_ALIGN_QW 24, r4, r5, r6, r7, r8
 	AVG_PW r7, r8
 	AVG_PW r6, r7
@@ -248,7 +248,7 @@ MC_put_x_16_arm_align3:
 	.internal MC_put_x_8_arm
 MC_put_x_8_arm:
 	@@ void func(uint8_t * dest, const uint8_t * ref, int stride, int height)
-	pld [r1]
+	@pld [r1]
         stmfd sp!, {r4-r11,lr} @ R14 is also called LR
 	ldr r11, 0f
 	and r4, r1, #3
@@ -274,7 +274,7 @@ MC_put_x_8_arm:
 MC_put_x_8_arm_align0:
 	ldmia r1, {r4-r6}
 	add r1, r1, r2
-	pld [r1]
+	@pld [r1]
 	AVG_PW r5, r6
 	AVG_PW r4, r5
 	stmia r0, {r5-r6}
@@ -286,7 +286,7 @@ MC_put_x_8_arm_align1:
 	and r1, r1, #0xFFFFFFFC
 1:	ldmia r1, {r4-r6}
 	add r1, r1, r2
-	pld [r1]
+	@pld [r1]
 	ADJ_ALIGN_DW 8, r4, r5, r6
 	AVG_PW r5, r6
 	AVG_PW r4, r5
@@ -299,7 +299,7 @@ MC_put_x_8_arm_align2:
 	and r1, r1, #0xFFFFFFFC
 1:	ldmia r1, {r4-r6}
 	add r1, r1, r2
-	pld [r1]
+	@pld [r1]
 	ADJ_ALIGN_DW 16, r4, r5, r6
 	AVG_PW r5, r6
 	AVG_PW r4, r5
@@ -312,7 +312,7 @@ MC_put_x_8_arm_align3:
 	and r1, r1, #0xFFFFFFFC
 1:	ldmia r1, {r4-r6}
 	add r1, r1, r2
-	pld [r1]
+	@pld [r1]
 	ADJ_ALIGN_DW 24, r4, r5, r6
 	AVG_PW r5, r6
 	AVG_PW r4, r5
